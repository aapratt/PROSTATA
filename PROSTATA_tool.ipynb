{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/mitiau/PROSTATA/blob/main/PROSTATA_tool.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f59Ujuujn___"
   },
   "source": [
    "# Install dependecies and download weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code provided according Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "apiUcTpNTnlU",
    "outputId": "d7491c10-dae0-4701-844d-1c76d0353a04",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install fair-esm\n",
    "!pip install biopython\n",
    "!pip install gdown==4.5.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bsyfz4BrSxMN"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive, files\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch import nn\n",
    "\n",
    "import transformers\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import esm\n",
    "from esm import ProteinBertModel\n",
    "from esm.pretrained import load_model_and_alphabet_hub\n",
    "\n",
    "from Bio import SeqIO\n",
    "from io import StringIO, BytesIO\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://a025generative-modeling-for-design.obs.ru-moscow-1.hc.sbercloud.ru/hse_protein_seminar/ESMForSingleMutationPosConcat\n",
    "!wget https://a025generative-modeling-for-design.obs.ru-moscow-1.hc.sbercloud.ru/hse_protein_seminar/ESMForSingleMutationPosOuter\n",
    "!wget https://a025generative-modeling-for-design.obs.ru-moscow-1.hc.sbercloud.ru/hse_protein_seminar/ESMForSingleMutation_cls\n",
    "!wget https://a025generative-modeling-for-design.obs.ru-moscow-1.hc.sbercloud.ru/hse_protein_seminar/ESMForSingleMutation_pos\n",
    "!wget https://a025generative-modeling-for-design.obs.ru-moscow-1.hc.sbercloud.ru/hse_protein_seminar/ESMForSingleMutation_pos_cat_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/mitiau/PROSTATA.git\n",
    "!git -C PROSTATA checkout HSE_seminar\n",
    "!git -C PROSTATA pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hYLdBMG7UUx3"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "HIDDEN_UNITS_POS_CONTACT = 5\n",
    "class ESMForSingleMutationPosConcat(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.esm2, _ = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "        self.fc1 = nn.Linear(1280 * 2, HIDDEN_UNITS_POS_CONTACT)\n",
    "        self.fc2 = nn.Linear(HIDDEN_UNITS_POS_CONTACT, 1)\n",
    "\n",
    "    def forward(self, token_ids1, token_ids2, pos):\n",
    "        outputs1 = self.esm2.forward(token_ids1, repr_layers=[33])[\n",
    "            'representations'][33]\n",
    "        outputs2 = self.esm2.forward(token_ids2, repr_layers=[33])[\n",
    "            'representations'][33]\n",
    "        outputs1_pos = outputs1[:, pos + 1]\n",
    "        outputs2_pos = outputs2[:, pos + 1]\n",
    "        outputs_pos_concat = torch.cat((outputs1_pos, outputs2_pos), 2)\n",
    "        fc1_outputs = F.relu(self.fc1(outputs_pos_concat))\n",
    "        logits = self.fc2(fc1_outputs)\n",
    "        return logits\n",
    "    \n",
    "HIDDEN_UNITS_POS_OUTER = 5\n",
    "class ESMForSingleMutationPosOuter(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.esm2, _ = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "        self._freeze_esm2_layers()\n",
    "        self.fc1 = nn.Linear(1280 * 1280, HIDDEN_UNITS_POS_OUTER)\n",
    "        self.fc2 = nn.Linear(HIDDEN_UNITS_POS_OUTER, 1)\n",
    "\n",
    "    def _freeze_esm2_layers(self):\n",
    "        total_blocks = 33\n",
    "        initial_layers = 2\n",
    "        layers_per_block = 16\n",
    "        num_freeze_blocks = total_blocks - 3\n",
    "        for _, param in list(self.esm2.named_parameters())[\n",
    "            :initial_layers + layers_per_block * num_freeze_blocks]:\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, token_ids1, token_ids2, pos):\n",
    "        outputs1 = self.esm2.forward(token_ids1, repr_layers=[33])[\n",
    "            'representations'][33]\n",
    "        outputs2 = self.esm2.forward(token_ids2, repr_layers=[33])[\n",
    "            'representations'][33]\n",
    "        outputs1_pos = outputs1[:, pos + 1]\n",
    "        outputs2_pos = outputs2[:, pos + 1]\n",
    "        outer_prod = outputs1_pos.unsqueeze(3) @ outputs2_pos.unsqueeze(2)\n",
    "        outer_prod_view = outer_prod.view(outer_prod.shape[0], outer_prod.shape[1], -1)\n",
    "        fc1_outputs = F.relu(self.fc1(outer_prod_view))\n",
    "        logits = self.fc2(fc1_outputs)\n",
    "        return logits\n",
    "    \n",
    "class ESMForSingleMutation_pos(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "        self.esm1v, self.esm1v_alphabet = esm.pretrained.esm2_t33_650M_UR50D()        \n",
    "        self.classifier = nn.Linear(1280, 1)\n",
    "        self.const1 = torch.nn.Parameter(torch.ones((1,1280)))\n",
    "        self.const2 = torch.nn.Parameter(-1 * torch.ones((1,1280)))\n",
    "        \n",
    "\n",
    "    def forward(self, token_ids1, token_ids2, pos):                \n",
    "        outputs1 = self.esm1v.forward(token_ids1, repr_layers=[33])['representations'][33]\n",
    "        outputs2 = self.esm1v.forward(token_ids2, repr_layers=[33])['representations'][33]\n",
    "        outputs = self.const1 * outputs1[:,pos + 1,:] + self.const2 * outputs2[:,pos + 1,:]        \n",
    "        logits = self.classifier(outputs)\n",
    "        return logits\n",
    "    \n",
    "class ESMForSingleMutation_cls(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "        self.esm1v, self.esm1v_alphabet = esm.pretrained.esm2_t33_650M_UR50D()        \n",
    "        self.classifier = nn.Linear(1280, 1)\n",
    "        self.const1 = torch.nn.Parameter(torch.ones((1,1280)))\n",
    "        self.const2 = torch.nn.Parameter(-1 * torch.ones((1,1280)))\n",
    "        \n",
    "\n",
    "    def forward(self, token_ids1, token_ids2, pos):                \n",
    "        outputs1 = self.esm1v.forward(token_ids1, repr_layers=[33])['representations'][33]\n",
    "        outputs2 = self.esm1v.forward(token_ids2, repr_layers=[33])['representations'][33]\n",
    "        outputs = self.const1 * outputs1[:,0,:] + self.const2 * outputs2[:,0,:]        \n",
    "        logits = self.classifier(outputs.unsqueeze(0))\n",
    "        return logits\n",
    "    \n",
    "class ESMForSingleMutation_pos_cat_cls(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "        self.esm1v, self.esm1v_alphabet = esm.pretrained.esm2_t33_650M_UR50D()        \n",
    "        self.classifier = nn.Linear(1280*2, 1)\n",
    "        self.const1 = torch.nn.Parameter(torch.ones((1,1280)))\n",
    "        self.const2 = torch.nn.Parameter(-1 * torch.ones((1,1280)))\n",
    "        \n",
    "\n",
    "    def forward(self, token_ids1, token_ids2, pos):                \n",
    "        outputs1 = self.esm1v.forward(token_ids1, repr_layers=[33])['representations'][33]\n",
    "        outputs2 = self.esm1v.forward(token_ids2, repr_layers=[33])['representations'][33]\n",
    "        cls_out = self.const1 * outputs1[:,0,:] + self.const2 * outputs2[:,0,:]\n",
    "        pos_out = self.const1 * outputs1[:,pos+1,:] + self.const2 * outputs2[:,pos+1,:]\n",
    "        outputs = torch.cat([cls_out.unsqueeze(0), pos_out], axis = -1)        \n",
    "        logits = self.classifier(outputs)\n",
    "        return logits\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N3Pjcm9HvRMC"
   },
   "outputs": [],
   "source": [
    "model_names = ['ESMForSingleMutationPosOuter',\n",
    "          'ESMForSingleMutationPosConcat',\n",
    "          'ESMForSingleMutation_pos_cat_cls',  \n",
    "              'ESMForSingleMutation_pos', \n",
    "              'ESMForSingleMutation_cls']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TAp3NQyaupxE"
   },
   "source": [
    "# Compute DeltaDDG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('ESMForSingleMutation_cls', map_location=torch.device('cpu'))\n",
    "esm2_alphabet = model.esm1v_alphabet\n",
    "esm2batch_converter = esm2_alphabet.get_batch_converter()\n",
    "\n",
    "def predict_ddg(seqs, mutation_codes, poss = None):\n",
    "    if poss is None:\n",
    "        poss = [None]*len(seqs)\n",
    "    inp = []\n",
    "    for seq, mutation_code, pos in zip(seqs, mutation_codes, poss):\n",
    "        wt_aa = mutation_code[0]\n",
    "        mut_aa = mutation_code[-1]\n",
    "        if pos:\n",
    "            mut_pos = pos\n",
    "        else:\n",
    "            mut_pos = int(mutation_code[1:-1])-1\n",
    "\n",
    "        assert seq[pos] == wt_aa\n",
    "        \n",
    "        wt = seq\n",
    "        tt = list(seq)\n",
    "        tt[mut_pos] = mut_aa\n",
    "        mut = ''.join(tt)\n",
    "\n",
    "    \n",
    "    \n",
    "        _, _, esm2_batch_tokens1 = esm2batch_converter([('' , wt[:1022])])\n",
    "        _, _, esm2_batch_tokens2 = esm2batch_converter([('' , mut[:1022])])\n",
    "        esm2_batch_tokens1 = esm2_batch_tokens1.cuda()\n",
    "        esm2_batch_tokens2 = esm2_batch_tokens2.cuda()\n",
    "    \n",
    "        inp.append((esm2_batch_tokens1, esm2_batch_tokens2, mut_pos))\n",
    "    \n",
    "    res = []\n",
    "    for model_name in model_names:\n",
    "        model = torch.load(model_name, map_location=torch.device('cpu'))\n",
    "        model.eval()\n",
    "        model.cuda()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            res.append([model(token_ids1 = t1, token_ids2 = t2, \n",
    "                             pos = torch.LongTensor([p])).cpu().numpy() for t1, t2, p in inp]\n",
    "        #print(f'Model {model_name} DDG prediction is {res[-1]}')\n",
    "    res = np.mean(res, axis = 0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>wt_seq</th>\n",
       "      <th>mut_seq</th>\n",
       "      <th>ddg</th>\n",
       "      <th>pdb_id</th>\n",
       "      <th>mut_info</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>VINTFDGVADYLQTYHKLPDNYITKSEAQALGWVASKGNLADVAPG...</td>\n",
       "      <td>VINTFDGVADYLQTYHKLPDNYITKSEAQGLGWVASKGNLADVAPG...</td>\n",
       "      <td>-0.90</td>\n",
       "      <td>1A2PA</td>\n",
       "      <td>A32G</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>VINTFDGVADYLQTYHKLPDNYITKSEAQALGWVASKGNLADVAPG...</td>\n",
       "      <td>VINTFDGVAAYLQTYHKLPDNYITKSEAQALGWVASKGNLADVAPG...</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>1A2PA</td>\n",
       "      <td>D12A</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>VINTFDGVADYLQTYHKLPDNYITKSEAQALGWVASKGNLADVAPG...</td>\n",
       "      <td>VINTFDGVAGYLQTYHKLPDNYITKSEAQALGWVASKGNLADVAPG...</td>\n",
       "      <td>-1.29</td>\n",
       "      <td>1A2PA</td>\n",
       "      <td>D12G</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>VINTFDGVADYLQTYHKLPDNYITKSEAQALGWVASKGNLADVAPG...</td>\n",
       "      <td>VINTFDGVADYLQTYHKLPDNYITKSEAQALGWVASKGNLADVAPG...</td>\n",
       "      <td>-2.97</td>\n",
       "      <td>1A2PA</td>\n",
       "      <td>D54A</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>VINTFDGVADYLQTYHKLPDNYITKSEAQALGWVASKGNLADVAPG...</td>\n",
       "      <td>VINTFDGVADYLQTYHKLPDNYITKSEAQALGWVASKGNLADVAPG...</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>1A2PA</td>\n",
       "      <td>D54N</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0  \\\n",
       "0             0           4   \n",
       "1             1           5   \n",
       "2             2           6   \n",
       "3             3           7   \n",
       "4             4           8   \n",
       "\n",
       "                                              wt_seq  \\\n",
       "0  VINTFDGVADYLQTYHKLPDNYITKSEAQALGWVASKGNLADVAPG...   \n",
       "1  VINTFDGVADYLQTYHKLPDNYITKSEAQALGWVASKGNLADVAPG...   \n",
       "2  VINTFDGVADYLQTYHKLPDNYITKSEAQALGWVASKGNLADVAPG...   \n",
       "3  VINTFDGVADYLQTYHKLPDNYITKSEAQALGWVASKGNLADVAPG...   \n",
       "4  VINTFDGVADYLQTYHKLPDNYITKSEAQALGWVASKGNLADVAPG...   \n",
       "\n",
       "                                             mut_seq   ddg pdb_id mut_info  \\\n",
       "0  VINTFDGVADYLQTYHKLPDNYITKSEAQGLGWVASKGNLADVAPG... -0.90  1A2PA     A32G   \n",
       "1  VINTFDGVAAYLQTYHKLPDNYITKSEAQALGWVASKGNLADVAPG... -0.31  1A2PA     D12A   \n",
       "2  VINTFDGVAGYLQTYHKLPDNYITKSEAQALGWVASKGNLADVAPG... -1.29  1A2PA     D12G   \n",
       "3  VINTFDGVADYLQTYHKLPDNYITKSEAQALGWVASKGNLADVAPG... -2.97  1A2PA     D54A   \n",
       "4  VINTFDGVADYLQTYHKLPDNYITKSEAQALGWVASKGNLADVAPG... -0.27  1A2PA     D54N   \n",
       "\n",
       "   pos  \n",
       "0   29  \n",
       "1    9  \n",
       "2    9  \n",
       "3   51  \n",
       "4   51  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('PROSTATA/cross_validation_datasets/test_1LNIA.csv')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['ddg_pred'] = predict_ddg(test_df['wt_seq'], \n",
    "                                  test_df['mut_info'], \n",
    "                                  test_df['pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = test_df.ddg.to_list()\n",
    "x = test_df.ddg_pred.to_list()\n",
    "plt.scatter(x, y,alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs = ['VINTFDGVADYLQTYHKLPDNYITKSEAQALGWVASKGNLADVAPGKSIGGDIFSNREGKLPGKSGRTWREADINYTSGFRNSDRILYSSDWLIYKTTDHYQTFTKIR']\n",
    "mutation_codes = ['V1N'] #@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_ddg(seqs, mutation_codes)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "Protein Stability Assessment using Transformers",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
