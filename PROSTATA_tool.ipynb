{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/mitiau/PROSTATA/blob/HSE_seminar/PROSTATA_tool.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f59Ujuujn___"
   },
   "source": [
    "# Install dependecies and download weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code provided according Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "apiUcTpNTnlU",
    "outputId": "d7491c10-dae0-4701-844d-1c76d0353a04",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install fair-esm\n",
    "!pip install biopython\n",
    "!pip install gdown==4.5.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bsyfz4BrSxMN"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive, files\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch import nn\n",
    "\n",
    "import transformers\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import esm\n",
    "from esm import ProteinBertModel\n",
    "from esm.pretrained import load_model_and_alphabet_hub\n",
    "\n",
    "from Bio import SeqIO\n",
    "from io import StringIO, BytesIO\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://a025generative-modeling-for-design.obs.ru-moscow-1.hc.sbercloud.ru/hse_protein_seminar/ESMForSingleMutationPosConcat\n",
    "!wget https://a025generative-modeling-for-design.obs.ru-moscow-1.hc.sbercloud.ru/hse_protein_seminar/ESMForSingleMutationPosOuter\n",
    "!wget https://a025generative-modeling-for-design.obs.ru-moscow-1.hc.sbercloud.ru/hse_protein_seminar/ESMForSingleMutation_cls\n",
    "!wget https://a025generative-modeling-for-design.obs.ru-moscow-1.hc.sbercloud.ru/hse_protein_seminar/ESMForSingleMutation_pos\n",
    "!wget https://a025generative-modeling-for-design.obs.ru-moscow-1.hc.sbercloud.ru/hse_protein_seminar/ESMForSingleMutation_pos_cat_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hYLdBMG7UUx3"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "HIDDEN_UNITS_POS_CONTACT = 5\n",
    "class ESMForSingleMutationPosConcat(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.esm2, _ = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "        self.fc1 = nn.Linear(1280 * 2, HIDDEN_UNITS_POS_CONTACT)\n",
    "        self.fc2 = nn.Linear(HIDDEN_UNITS_POS_CONTACT, 1)\n",
    "\n",
    "    def forward(self, token_ids1, token_ids2, pos):\n",
    "        outputs1 = self.esm2.forward(token_ids1, repr_layers=[33])[\n",
    "            'representations'][33]\n",
    "        outputs2 = self.esm2.forward(token_ids2, repr_layers=[33])[\n",
    "            'representations'][33]\n",
    "        outputs1_pos = outputs1[:, pos + 1]\n",
    "        outputs2_pos = outputs2[:, pos + 1]\n",
    "        outputs_pos_concat = torch.cat((outputs1_pos, outputs2_pos), 2)\n",
    "        fc1_outputs = F.relu(self.fc1(outputs_pos_concat))\n",
    "        logits = self.fc2(fc1_outputs)\n",
    "        return logits\n",
    "    \n",
    "HIDDEN_UNITS_POS_OUTER = 5\n",
    "class ESMForSingleMutationPosOuter(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.esm2, _ = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "        self._freeze_esm2_layers()\n",
    "        self.fc1 = nn.Linear(1280 * 1280, HIDDEN_UNITS_POS_OUTER)\n",
    "        self.fc2 = nn.Linear(HIDDEN_UNITS_POS_OUTER, 1)\n",
    "\n",
    "    def _freeze_esm2_layers(self):\n",
    "        total_blocks = 33\n",
    "        initial_layers = 2\n",
    "        layers_per_block = 16\n",
    "        num_freeze_blocks = total_blocks - 3\n",
    "        for _, param in list(self.esm2.named_parameters())[\n",
    "            :initial_layers + layers_per_block * num_freeze_blocks]:\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, token_ids1, token_ids2, pos):\n",
    "        outputs1 = self.esm2.forward(token_ids1, repr_layers=[33])[\n",
    "            'representations'][33]\n",
    "        outputs2 = self.esm2.forward(token_ids2, repr_layers=[33])[\n",
    "            'representations'][33]\n",
    "        outputs1_pos = outputs1[:, pos + 1]\n",
    "        outputs2_pos = outputs2[:, pos + 1]\n",
    "        outer_prod = outputs1_pos.unsqueeze(3) @ outputs2_pos.unsqueeze(2)\n",
    "        outer_prod_view = outer_prod.view(outer_prod.shape[0], outer_prod.shape[1], -1)\n",
    "        fc1_outputs = F.relu(self.fc1(outer_prod_view))\n",
    "        logits = self.fc2(fc1_outputs)\n",
    "        return logits\n",
    "    \n",
    "class ESMForSingleMutation_pos(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "        self.esm1v, self.esm1v_alphabet = esm.pretrained.esm2_t33_650M_UR50D()        \n",
    "        self.classifier = nn.Linear(1280, 1)\n",
    "        self.const1 = torch.nn.Parameter(torch.ones((1,1280)))\n",
    "        self.const2 = torch.nn.Parameter(-1 * torch.ones((1,1280)))\n",
    "        \n",
    "\n",
    "    def forward(self, token_ids1, token_ids2, pos):                \n",
    "        outputs1 = self.esm1v.forward(token_ids1, repr_layers=[33])['representations'][33]\n",
    "        outputs2 = self.esm1v.forward(token_ids2, repr_layers=[33])['representations'][33]\n",
    "        outputs = self.const1 * outputs1[:,pos + 1,:] + self.const2 * outputs2[:,pos + 1,:]        \n",
    "        logits = self.classifier(outputs)\n",
    "        return logits\n",
    "    \n",
    "class ESMForSingleMutation_cls(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "        self.esm1v, self.esm1v_alphabet = esm.pretrained.esm2_t33_650M_UR50D()        \n",
    "        self.classifier = nn.Linear(1280, 1)\n",
    "        self.const1 = torch.nn.Parameter(torch.ones((1,1280)))\n",
    "        self.const2 = torch.nn.Parameter(-1 * torch.ones((1,1280)))\n",
    "        \n",
    "\n",
    "    def forward(self, token_ids1, token_ids2, pos):                \n",
    "        outputs1 = self.esm1v.forward(token_ids1, repr_layers=[33])['representations'][33]\n",
    "        outputs2 = self.esm1v.forward(token_ids2, repr_layers=[33])['representations'][33]\n",
    "        outputs = self.const1 * outputs1[:,0,:] + self.const2 * outputs2[:,0,:]        \n",
    "        logits = self.classifier(outputs.unsqueeze(0))\n",
    "        return logits\n",
    "    \n",
    "class ESMForSingleMutation_pos_cat_cls(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "        self.esm1v, self.esm1v_alphabet = esm.pretrained.esm2_t33_650M_UR50D()        \n",
    "        self.classifier = nn.Linear(1280*2, 1)\n",
    "        self.const1 = torch.nn.Parameter(torch.ones((1,1280)))\n",
    "        self.const2 = torch.nn.Parameter(-1 * torch.ones((1,1280)))\n",
    "        \n",
    "\n",
    "    def forward(self, token_ids1, token_ids2, pos):                \n",
    "        outputs1 = self.esm1v.forward(token_ids1, repr_layers=[33])['representations'][33]\n",
    "        outputs2 = self.esm1v.forward(token_ids2, repr_layers=[33])['representations'][33]\n",
    "        cls_out = self.const1 * outputs1[:,0,:] + self.const2 * outputs2[:,0,:]\n",
    "        pos_out = self.const1 * outputs1[:,pos+1,:] + self.const2 * outputs2[:,pos+1,:]\n",
    "        outputs = torch.cat([cls_out.unsqueeze(0), pos_out], axis = -1)        \n",
    "        logits = self.classifier(outputs)\n",
    "        return logits\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N3Pjcm9HvRMC"
   },
   "outputs": [],
   "source": [
    "model_names = ['ESMForSingleMutationPosOuter',\n",
    "          'ESMForSingleMutationPosConcat',\n",
    "          'ESMForSingleMutation_pos_cat_cls',  \n",
    "              'ESMForSingleMutation_pos', \n",
    "              'ESMForSingleMutation_cls']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TAp3NQyaupxE"
   },
   "source": [
    "# Compute DeltaDDG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "urrTMPfMUrbP"
   },
   "outputs": [],
   "source": [
    "uploaded = files.upload()\n",
    "for fn in uploaded.keys():\n",
    "    print('User uploaded file \"{name}\" with length {length} bytes'.format(name=fn, length=len(uploaded[fn])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in uploaded.keys():\n",
    "    identifiers = []\n",
    "    seqs = []\n",
    "    for seq_record in SeqIO.parse(StringIO(BytesIO(uploaded[key]).read().decode('UTF-8')), 'fasta'):  # (generator)\n",
    "        identifiers.append(seq_record.id)\n",
    "        seqs.append(seq_record.seq)\n",
    "#Only using the first sequence from uploaded files\n",
    "seq, identifier = seqs[0], identifiers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutation_code = 'Q9H' #@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ddg(seq, mutation_code, pos = None):\n",
    "    wt_aa = mutation_code[0]\n",
    "    mut_aa = mutation_code[-1]\n",
    "    if pos:\n",
    "        mut_pos = pos\n",
    "    else:\n",
    "        mut_pos = int(mutation_code[1:-1])-1\n",
    "\n",
    "    assert seq[pos] == wt_aa\n",
    "        \n",
    "    wt = seq\n",
    "    tt = list(seq)\n",
    "    tt[mut_pos] = mut_aa\n",
    "    mut = ''.join(tt)\n",
    "\n",
    "    model = torch.load('ESMForSingleMutation_cls', map_location=torch.device('cpu'))\n",
    "    esm2_alphabet = model.esm1v_alphabet\n",
    "    esm2batch_converter = esm2_alphabet.get_batch_converter()\n",
    "    _, _, esm2_batch_tokens1 = esm2batch_converter([('' , wt[:1022])])\n",
    "    _, _, esm2_batch_tokens2 = esm2batch_converter([('' , mut[:1022])])\n",
    "    esm2_batch_tokens1 = esm2_batch_tokens1.cuda()\n",
    "    esm2_batch_tokens2 = esm2_batch_tokens2.cuda()\n",
    "    \n",
    "    res=[]\n",
    "    for model_name in model_names:\n",
    "        model = torch.load(model_name, map_location=torch.device('cpu'))\n",
    "        model.eval()\n",
    "        model.cuda()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            res.append(model(token_ids1 = esm2_batch_tokens1, \n",
    "                             token_ids2 = esm2_batch_tokens2, \n",
    "                             pos = torch.LongTensor([mut_pos])).cpu().numpy())\n",
    "        print(f'Model {model_name} DDG prediction is {res[-1]}')\n",
    "    res = np.mean(res)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('cross_validation_datasets/test_1LNIA.csv')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head().apply(lambda row: predict_ddg(row['wt_seq'], row['mut_info'], row['pos']), axis  = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = test_df.ddg.to_list()\n",
    "x = test_df.ddg_pred.to_list()\n",
    "plt.scatter(x, y,alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = 'VINTFDGVADYLQTYHKLPDNYITKSEAQALGWVASKGNLADVAPGKSIGGDIFSNREGKLPGKSGRTWREADINYTSGFRNSDRILYSSDWLIYKTTDHYQTFTKIR'\n",
    "mutation_code = 'V1N' #@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_ddg(seq, mutation_code)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "Protein Stability Assessment using Transformers",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
